---
title: "Databricks Connect å®Ÿè·µç·¨ - ãƒ­ãƒ¼ã‚«ãƒ«ã‹ã‚‰ Databricks ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒˆã‚’åˆ©ç”¨"
emoji: "ğŸš€"
type: "tech"
topics:
  - "databricks"
  - "python"
  - "spark"
  - "devcontainer"
published: true
published_at: 2025-08-06 19:02
publication_name: "genda_jp"
---

æœ¬è¨˜äº‹ã¯3éƒ¨æ§‹æˆã®3æœ¬ç›®ã§ã™ã€‚

1. [Claude Code å¯¾å¿œ Dev Container ç’°å¢ƒæ§‹ç¯‰ç·¨ - VS Code ã§ã‚‚ãã‚Œä»¥å¤–ã§ã‚‚](2025-08-06-devcontainer-claude-code)
2. [Python é–‹ç™ºç’°å¢ƒæœ€é©åŒ–ç·¨ - uv + pre-commit + GitHub Actions](2025-08-06-python-env-optimization)
3. Databricks Connect å®Ÿè·µç·¨ - ãƒ­ãƒ¼ã‚«ãƒ«ã‹ã‚‰ Databricks ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒˆã‚’åˆ©ç”¨

## 1. ã¯ã˜ã‚ã«

å‰å›ã¾ã§ã®ç’°å¢ƒã« Databricks Connect ã‚’è¿½åŠ ã—ã¦ã€ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç’°å¢ƒã‹ã‚‰ Databricks ã® Spark ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’åˆ©ç”¨ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚

Databricks ä¸Šã§ã‚‚ãƒ­ãƒ¼ã‚«ãƒ«ã§ã‚‚åŒã˜ã‚³ãƒ¼ãƒ‰ãŒå‹•ä½œã™ã‚‹æ±ç”¨çš„ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚‚ç´¹ä»‹ã—ã¾ã™ã€‚

## 2. å¯¾è±¡èª­è€…

- Databricks ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒˆã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã‹ã‚‰åˆ©ç”¨ã—ãŸã„æ–¹
- Databricks ä¸Šã§ã®é–‹ç™ºã« Claude Code ã‚’åˆ©ç”¨ã—ãŸã„æ–¹
- Databricks ä»¥å¤–ã® DWH ã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½¿ã£ã¦ã„ã‚‹ãŒå‚è€ƒã«ã—ãŸã„æ–¹

## 3. Databricks Connect ã¨ã¯

Databricks Connect ã¯ã€ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç’°å¢ƒã‹ã‚‰ Databricks ã‚¯ãƒ©ã‚¹ã‚¿ã«æ¥ç¶šã§ãã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚

ã“ã‚Œã«ã‚ˆã‚Šä»¥ä¸‹ã®ãƒ¡ãƒªãƒƒãƒˆãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚

- VS Code ã‚„ Claude Code ãªã©ã®ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç’°å¢ƒãŒåˆ©ç”¨ã§ãã‚‹
- ãƒ­ãƒ¼ã‚«ãƒ«ã§ã®é–‹ç™ºã¨ Databricks ä¸Šã§ã®å®Ÿè¡Œã§åŒã˜ã‚³ãƒ¼ãƒ‰ãŒä½¿ãˆã‚‹

## 4. ç’°å¢ƒè¨­å®š

### 4.1. pyproject.toml ã¸ã®è¿½åŠ 

å‰å›ã® pyproject.toml ã«ä»¥ä¸‹ã®ä¾å­˜é–¢ä¿‚ã‚’è¿½åŠ ã—ã¾ã™ã€‚

```toml:pyproject.toml
[project]
dependencies = [
    "databricks-connect~=16.4.0",
    "ipykernel",
    "jupyterlab",
    "matplotlib",
    "numpy",
    "pandas",
    "python-dotenv",
    "requests",
    "seaborn",
]

[tool.flake8]
extend-ignore = [
    "E203",  # Whitespace before ':'
    "E701",  # Multiple statements on one line (colon)
    "F821"   # undefined name (Databricks-specific module)
]
```

### 4.2. Dev Containerè¨­å®šã®æ›´æ–°

devcontainer.json ã«ä»¥ä¸‹ã®è¨­å®šã‚’è¿½åŠ ã—ã¾ã™ã€‚

```json:.devcontainer/devcontainer.json
{
    "runArgs": [
        "--cap-add=NET_ADMIN",
        "--cap-add=NET_RAW",
        "--network=host"
    ],
    "mounts": [
        "source=${localEnv:HOME}/.databrickscfg,target=/home/node/.databrickscfg,type=bind,consistency=cached",
    ]
}
```

### 4.3. ç’°å¢ƒå¤‰æ•°è¨­å®š

`.env.example` ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚

```sh:.env.example
# Databricksè¨­å®š

# .databrickscfgã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆæ¨å¥¨ï¼‰
DATABRICKS_CONFIG_PROFILE=DEFAULT

# ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ä½¿ç”¨ã®å ´åˆï¼ˆã©ã¡ã‚‰ã‹ä¸€æ–¹ã‚’è¨­å®šï¼‰
# DATABRICKS_CLUSTER_ID=

# Serverless Computeä½¿ç”¨ã®å ´åˆ
DATABRICKS_SERVERLESS_COMPUTE_ID=auto

# ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãƒã‚§ãƒƒã‚¯ç„¡åŠ¹åŒ–
DATABRICKS_CONNECT_DISABLE_VERSION_CHECK=true
```

## 5. Spark ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒª

Dev Container (ãƒ­ãƒ¼ã‚«ãƒ«) ã¨ Databricks ã®ä¸¡æ–¹ã§åŒã˜ã‚ˆã†ã« Spark ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆã™ã‚‹ãŸã‚ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ç”¨æ„ã—ã¾ã—ãŸã€‚

### 5.1. ä½¿ç”¨æ–¹æ³•

```python
from databricks_spark import create_spark_session

# æ–°ã—ã„Sparkã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
spark = create_spark_session()
df = spark.sql("SHOW CATALOGS")
df.show()
```

### 5.2. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªå®Ÿè£…

:::details databricks_setup.py

```python:databricks_setup.py
"""
Databricksçµ±ä¸€ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£

ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¨ã‚¸ãƒ§ãƒ–ã§åŒã˜ã‚³ãƒ¼ãƒ‰ã§Databricksç’°å¢ƒã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
Sparkã€dbutilsã€displayé–¢æ•°ã‚’ç’°å¢ƒã«å¿œã˜ã¦è‡ªå‹•åˆæœŸåŒ–

ä½¿ç”¨æ–¹æ³•:
    from databricks_setup import setup_spark, setup_dbutils_mock, setup_display_function

    spark = setup_spark()
    dbutils = setup_dbutils_mock()
    display = setup_display_function()

æ©Ÿèƒ½:
- ç’°å¢ƒè‡ªå‹•åˆ¤å®šï¼ˆDatabricks vs ãƒ­ãƒ¼ã‚«ãƒ«ï¼‰
- Sparkã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
- dbutilsãƒ¢ãƒƒã‚¯ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒç”¨ï¼‰
- displayé–¢æ•°ä»£æ›¿ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒç”¨ï¼‰
"""

import logging
import os
import sys
from pathlib import Path

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logger = logging.getLogger(__name__)
logger.handlers.clear()
logger.propagate = False
logger.setLevel(logging.INFO)
formatter = logging.Formatter("%(asctime)s %(name)s [%(levelname)s] %(message)s")

handler = logging.StreamHandler(sys.stdout)
handler.setFormatter(formatter)
logger.addHandler(handler)


def setup_spark():
    """ç’°å¢ƒã«å¿œã˜ã¦Sparkã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆãƒ»å–å¾—

    Databricksç’°å¢ƒã§ã¯æ—¢å­˜ã®sparkã‚’ä½¿ç”¨ã—ã€
    ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ã¯Databricks Connectã§ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’æ–°è¦ä½œæˆ

    Returns:
        SparkSession: ä½¿ç”¨å¯èƒ½ãªSparkã‚»ãƒƒã‚·ãƒ§ãƒ³
    """
    # å®Ÿè¡Œç’°å¢ƒã®æ¤œå‡º
    if os.environ.get("DATABRICKS_RUNTIME_VERSION"):
        logger.info("ğŸ” å®Ÿè¡Œç’°å¢ƒæ¤œå‡º: Databricks Runtime")
        logger.info(
            f"Databricksç’°å¢ƒã‚’æ¤œå‡º (Runtime: {os.environ.get('DATABRICKS_RUNTIME_VERSION')})"
        )
    else:
        logger.info("ğŸ” å®Ÿè¡Œç’°å¢ƒæ¤œå‡º: local")

    logger.info("Sparkã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚’é–‹å§‹")

    # Databricksç’°å¢ƒã®å ´åˆã¯ä½•ã‚‚ã—ãªã„ï¼ˆæ—¢å­˜ã®sparkã‚’ä½¿ç”¨ï¼‰
    if os.environ.get("DATABRICKS_RUNTIME_VERSION"):
        spark = globals().get("spark")
        logger.info("âœ… æ—¢å­˜ã®Sparkã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨")
        return spark

    # ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã®å ´åˆã¯databricks_sessionã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    logger.info("ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã‚’æ¤œå‡ºã€Databricks Connectã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—")
    if "__file__" in globals():
        current_dir = Path(__file__).parent
    else:
        current_dir = Path.cwd()

    project_root = current_dir
    while project_root.parent != project_root:
        if (project_root / ".devcontainer").exists():
            break
        project_root = project_root.parent

    devcontainer_path = project_root / ".devcontainer"
    if str(devcontainer_path) not in sys.path:
        sys.path.insert(0, str(devcontainer_path))

    from databricks_session import create_spark_session

    logger.info("Databricks Connectã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆä¸­...")
    spark = create_spark_session()
    logger.info("Sparkã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ä½œæˆå®Œäº†")
    return spark


def setup_dbutils_mock():
    """ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒç”¨ã®dbutilsãƒ¢ãƒƒã‚¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ

    Databricksç’°å¢ƒã®dbutilsã¨åŒã˜ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’æä¾›
    - widgets.getAll(): ç©ºã®è¾æ›¸ã‚’è¿”ã™
    - secrets.get(): ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å€¤ã‚’å–å¾—

    Returns:
        MockDbutils: dbutilsã®ãƒ¢ãƒƒã‚¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
    """

    class MockDbutils:
        class Widgets:
            def getAll(self):
                return {}

        class Secrets:
            def get(self, scope, key):
                env_var = f"DATABRICKS_SECRET_{scope.upper()}_{key.upper()}".replace(
                    "-", "_"
                )
                return os.environ.get(env_var, "")

        def __init__(self):
            self.widgets = self.Widgets()
            self.secrets = self.Secrets()

    return MockDbutils()


def setup_display_function():
    """ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒç”¨ã®displayé–¢æ•°ä»£æ›¿ã‚’ä½œæˆ

    DataFrameã‚’æ•´å½¢ã—ã¦æ¨™æº–å‡ºåŠ›ã«è¡¨ç¤º
    Databricksç’°å¢ƒã®display()ã¨åŒæ§˜ã®æ©Ÿèƒ½ã‚’æä¾›

    Returns:
        function: displayé–¢æ•°ã®ä»£æ›¿
    """

    def display(df):
        print("\n=== DataFrame ===")
        if df.empty:
            print("ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        else:
            print(df.to_string())
        print()

    return display
```

:::

:::details databricks_session.py

```python:databricks_session.py
"""
Databricks Sparkã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†

ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ã®Databricks Connectã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæ©Ÿèƒ½

ä½¿ç”¨æ–¹æ³•:
    from databricks_session import create_spark_session
    spark = create_spark_session()
    df = spark.sql("SHOW CATALOGS")
    df.show()
"""

import logging
import os
import sys

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logger = logging.getLogger(__name__)
logger.handlers.clear()
logger.propagate = False
logger.setLevel(logging.INFO)
formatter = logging.Formatter("%(asctime)s %(name)s [%(levelname)s] %(message)s")

handler = logging.StreamHandler(sys.stdout)
handler.setFormatter(formatter)
logger.addHandler(handler)


def create_spark_session():
    """Databricks Connectã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ

    .databrickscfgã¾ãŸã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰èªè¨¼æƒ…å ±ã‚’å–å¾—ã—ã€
    Databricksã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã¾ãŸã¯Serverless Computeã«æ¥ç¶š

    Returns:
        SparkSession: Databricks Connectã‚»ãƒƒã‚·ãƒ§ãƒ³

    Raises:
        ImportError: databricks-connectãŒåˆ©ç”¨ã§ããªã„å ´åˆ
        ValueError: å¿…è¦ãªç’°å¢ƒå¤‰æ•°ãŒæœªè¨­å®šã®å ´åˆ
    """
    logger.info("Databricks Connectã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ä½œæˆã‚’é–‹å§‹")
    try:
        from databricks.connect import DatabricksSession

        # ç’°å¢ƒå¤‰æ•°ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
        os.environ.setdefault("DATABRICKS_CONFIG_PROFILE", "DEFAULT")
        os.environ.setdefault("DATABRICKS_SERVERLESS_COMPUTE_ID", "auto")
        os.environ.setdefault("DATABRICKS_CONNECT_DISABLE_VERSION_CHECK", "true")

        profile = os.environ.get("DATABRICKS_CONFIG_PROFILE")
        logger.info(f"ç’°å¢ƒå¤‰æ•°: DATABRICKS_CONFIG_PROFILE={profile}")
        serverless_id = os.environ.get("DATABRICKS_SERVERLESS_COMPUTE_ID")
        logger.info(f"ç’°å¢ƒå¤‰æ•°: DATABRICKS_SERVERLESS_COMPUTE_ID={serverless_id}")

        # .envèª­ã¿è¾¼ã¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        try:
            from dotenv import find_dotenv, load_dotenv

            env_file = find_dotenv()
            if env_file:
                logger.info(f"ğŸ“ .envèª­ã¿è¾¼ã¿: {env_file}")
                load_dotenv(env_file)
        except ImportError:
            pass

        # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«æŒ‡å®šãƒã‚§ãƒƒã‚¯
        profile_name = os.environ.get("DATABRICKS_CONFIG_PROFILE")
        logger.info(f"ğŸ”§ .databrickscfgãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ä½¿ç”¨: {profile_name}")

        if profile_name:
            if os.environ.get("DATABRICKS_CLUSTER_ID"):
                cluster_id = os.environ.get("DATABRICKS_CLUSTER_ID")
                logger.info(f"ğŸ”— ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ID {cluster_id} ã«æ¥ç¶š")
                spark = (
                    DatabricksSession.builder.profile(profile_name)
                    .clusterId(cluster_id)
                    .getOrCreate()
                )
            else:
                logger.info("ğŸš€ Serverless Computeä½¿ç”¨")
                spark = (
                    DatabricksSession.builder.profile(profile_name)
                    .serverless(True)
                    .getOrCreate()
                )
        else:
            required_vars = ["DATABRICKS_HOST", "DATABRICKS_TOKEN"]
            missing_vars = [var for var in required_vars if not os.environ.get(var)]

            if missing_vars:
                raise ValueError(f"ç’°å¢ƒå¤‰æ•°ãŒæœªè¨­å®š: {missing_vars}")

            if os.environ.get("DATABRICKS_CLUSTER_ID"):
                cluster_id = os.environ.get("DATABRICKS_CLUSTER_ID")
                spark = DatabricksSession.builder.clusterId(cluster_id).getOrCreate()
            else:
                spark = DatabricksSession.builder.serverless(True).getOrCreate()

        # DataFrameè¡¨ç¤ºæœ€é©åŒ–ï¼ˆServerlessç’°å¢ƒã§ã¯ä¸€éƒ¨è¨­å®šãŒåˆ¶é™ã•ã‚Œã‚‹ï¼‰
        try:
            spark.conf.set("spark.sql.repl.eagerEval.enabled", True)
        except Exception:
            pass

        logger.info("âœ… Databricks Connect Sparkã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆå®Œäº†")

        # Sparkæƒ…å ±ã®è¡¨ç¤º
        try:
            spark_version = spark.version
            logger.info(f"ğŸ“Š Spark version: {spark_version}")

            # æ¥ç¶šå…ˆæƒ…å ±
            host = os.environ.get("DATABRICKS_HOST", "")
            if host:
                logger.info(f"ğŸŒ æ¥ç¶šå…ˆ: {host}")
        except Exception:
            pass

        return spark

    except ImportError as e:
        raise ImportError(f"databricks-connectãŒåˆ©ç”¨ã§ãã¾ã›ã‚“: {e}")
```

:::

## 6. Databrickså´ã®è¨­å®š

### 6.1. æ¥ç¶šå…ˆã‚¯ãƒ©ã‚¹ã‚¿è¨­å®š

Spark config ã§ä»¥ä¸‹ã‚’è¨­å®šã—ã¾ã™ã€‚

```properties:Spark config
spark.databricks.service.server.enabled true
```

### 6.2. Databricks æ¥ç¶šè¨­å®š

Databricks ã¸æ¥ç¶šã™ã‚‹å ´åˆã¯ `~/.databrickscfg` ã«ä»¥ä¸‹ã®å†…å®¹ã‚’è¨˜è¿°ã—ã¾ã™ã€‚

```ini:~/.databrickscfg
[DEFAULT]
host = https://your-databricks-workspace.cloud.databricks.com
token = your-access-token
```

- `host`: Databricks ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã® URL
- `token`: Databricks ã‚¢ã‚¯ã‚»ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³

## 7. åˆ©ç”¨æ‰‹é †

### 7.1. `.env` ä½œæˆ

1. `.env` ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã™

    ```sh
    cp .env.example .env
    ```

2. å¿…è¦ã§ã‚ã‚Œã° `.env` ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’å¤‰æ›´ã—ã¦ãã ã•ã„

### 7.2. é–‹ç™ºãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼

1. Dev Container ã‚’èµ·å‹•
2. Python ã‚«ãƒ¼ãƒãƒ«ã‚’é¸æŠ
3. Spark ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆã—ã¦é–‹ç™ºé–‹å§‹

```python
# .ipynb & .py (Databricks Notebook) å…±é€šã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
import os
import sys
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’æ¢ã™ (.devcontainer ã¾ãŸã¯ .git ã‚’åŸºæº–ã«)
project_root = None
for parent in [Path.cwd()] + list(Path.cwd().parents):
    # .devcontainer ã¾ãŸã¯ .git ãŒå­˜åœ¨ã™ã‚‹å ´æ‰€ã‚’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã¨ã™ã‚‹
    if (parent / '.devcontainer').exists() or (parent / '.git').exists():
        project_root = parent
        break

# Databricksç’°å¢ƒã®åˆ¤å®š
if not os.environ.get("DATABRICKS_RUNTIME_VERSION"):
    # === ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã®å ´åˆ ===
    # Databricks Connect ã‚’ä½¿ç”¨ã—ã¦ãƒªãƒ¢ãƒ¼ãƒˆã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã«æ¥ç¶šã™ã‚‹
    if project_root and (project_root / '.devcontainer').exists():
        sys.path.insert(0, str(project_root / '.devcontainer'))
        from databricks_setup import setup_spark, setup_dbutils_mock, setup_display_function

        spark = setup_spark()
        dbutils = setup_dbutils_mock()
        display = setup_display_function()
    else:
        raise FileNotFoundError("ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã¾ãŸã¯ .devcontainer ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
else:
    # === Databricks ç’°å¢ƒã®å ´åˆ ===
    # spark, dbutils ã¯æ—¢ã«å®šç¾©æ¸ˆã¿ãªã®ã§ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã®ã¿
    if project_root and (project_root / 'pyproject.toml').exists():
        # NOTE: uv ã¯ DBR ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹
        %pip install -r <(uv pip compile {project_root}/pyproject.toml --color never)
        %restart_python
    else:
        print("âš ï¸ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã¾ãŸã¯pyproject.tomlãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
```

æ›¸ãå‘³ãŒã‚ã¾ã‚Šè‰¯ããªã„ã®ã§æ”¹å–„ãƒã‚¤ãƒ³ãƒˆã§ã™ã€‚ã€‚

## 8. ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã® Python ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ–¹æ³•

ä¸Šã®å‚è€ƒã‚³ãƒ¼ãƒ‰ã«ã‚‚è¨˜è¼‰ã‚ã‚Šã¾ã™ãŒ Databricks ã§ã®å®Ÿè¡Œæ™‚ã®ã¿ Python ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ HACK ãªæ–¹æ³•ã§ã™ã€‚

uv ã‚’ä½¿ã£ã¦ã„ãªã„å ´åˆã¯ `%pip install <package>` ç­‰ã§å¤§ä¸ˆå¤«ã§ã™ã€‚

```python
import os

if os.environ.get("DATABRICKS_RUNTIME_VERSION"):
    # NOTE: uv ã¯ DBR ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹
    %pip install -r <(uv pip compile pyproject.toml --color never)
```

## 9. ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### 9.1. ãƒãƒ¼ã‚¸ãƒ§ãƒ³ä¸æ•´åˆã‚¨ãƒ©ãƒ¼

Databricks Connect ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨ã‚¯ãƒ©ã‚¹ã‚¿ã®ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒåˆã‚ãªã„å ´åˆãŒã‚ã‚Šã¾ã™

```sh
# ã‚¯ãƒ©ã‚¹ã‚¿ã®ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã«åˆã‚ã›ã¦ãƒ€ã‚¦ãƒ³ã‚°ãƒ¬ãƒ¼ãƒ‰
uv add 'databricks-connect~=14.3.0'  # ã“ã®å ´åˆã¯ DBR 14.3 ã«å¯¾å¿œ
```

### 9.2. æ¥ç¶šã‚¨ãƒ©ãƒ¼

è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã›ãšãƒã‚¦ãƒ³ãƒˆã•ã‚Œã¦ã„ãªã„å ´åˆãŒè€ƒãˆã‚‰ã‚Œã‚‹ã®ã§ã€devcontainer.json ã® `mounts` è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚


## 10. ãŠã‚ã‚Šã«

ã“ã‚Œã§ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç’°å¢ƒã‹ã‚‰ Databricks ã® Spark ã‚¯ãƒ©ã‚¹ã‚¿ã‚’åˆ©ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚

Claude Code ã«ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å®Ÿè¡Œã¨ãƒ‡ãƒãƒƒã‚°ã‚’ä»»ã›ã‚‹ã“ã¨ã§ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã‚„æ©Ÿæ¢°å­¦ç¿’ã®ä½œæ¥­åŠ¹ç‡ãŒçˆ†ä¸ŠãŒã‚Šã§ã™ã­ï¼

Claude Code ã«ã“ã®æ‰‹ã®ä½œæ¥­ã‚’ä»»ã›ã‚‹ã¨ãã¯æ™‚é–“ãŒã‹ã‹ã‚‹ã®ã§ä¸¦è¡Œã—ã¦ä»–ã®ä½œæ¥­ã«å–ã‚Šçµ„ã‚€ã®ãŒã‚ªã‚¹ã‚¹ãƒ¡ã§ã™ã€‚

## 11. (ãŠã¾ã‘) Claude Code ã«èª­ã¾ã›ã‚‹ã¨ä¾¿åˆ©ãªãƒ«ãƒ¼ãƒ«

```md:CONTRIBUTING.md
# CONTRIBUTING

## é‡è¦ãªãƒ«ãƒ¼ãƒ«

### pre-commit è¨­å®šã«ã¤ã„ã¦

- **NEVER**: pre-commit ã‚’ç„¡åŠ¹åŒ–ã—ã¦ã¯ãªã‚‰ãªã„
- **NEVER**: `pre-commit skip` ã‚„ `git commit --no-verify` ã‚’ä½¿ç”¨ã—ã¦ã¯ãªã‚‰ãªã„
- **IMPORTANT**: pre-commit ã®ãƒã‚§ãƒƒã‚¯ã«å¤±æ•—ã—ãŸå ´åˆã¯ã€å¿…ãšã‚³ãƒ¼ãƒ‰ã‚’ä¿®æ­£ã—ã¦ã‹ã‚‰ã‚³ãƒŸãƒƒãƒˆã™ã‚‹

## Jupyter Notebook å®Ÿè¡Œæ–¹æ³•

### ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å®Ÿè¡Œæ–¹æ³•

Notebookå…¨ä½“ã‚’å®Ÿè¡Œã™ã‚‹æŒ‡ç¤ºã‚’å—ã‘ãŸéš›ã¯ã€ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹

`uv run jupyter nbconvert --to notebook --execute <notebook_path> --inplace --ExecutePreprocessor.timeout=300`

#### ä½¿ç”¨ä¾‹

`uv run jupyter nbconvert --to notebook --execute /workspace/notebooks/databricks-connect-sample.ipynb --inplace --ExecutePreprocessor.timeout=300`

#### ã‚ªãƒ—ã‚·ãƒ§ãƒ³èª¬æ˜

- `--to notebook`: Notebookå½¢å¼ã§å‡ºåŠ›
- `--execute`: ã‚»ãƒ«ã‚’å®Ÿéš›ã«å®Ÿè¡Œ
- `--inplace`: å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«ã«å®Ÿè¡Œçµæœã‚’ä¸Šæ›¸ã
- `--ExecutePreprocessor.timeout=300`: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’300ç§’ã«è¨­å®š

### å®Ÿè¡Œãƒ­ã‚°ã®ç¢ºèª

å®Ÿè¡Œæ™‚ã®ãƒ­ã‚°ã‚’ç¢ºèªã—ãŸã„å ´åˆã¯ä»¥ä¸‹ã®ã‚ˆã†ã«å®Ÿè¡Œã™ã‚‹

`uv run jupyter nbconvert --to notebook --execute <notebook_path> --inplace --ExecutePreprocessor.timeout=300 2>&1 | tee /tmp/notebook_execution.log`

### æ³¨æ„äº‹é …

- å®Ÿè¡Œå‰ã«å¿…è¦ãªç’°å¢ƒå¤‰æ•°ï¼ˆ`.env`ãƒ•ã‚¡ã‚¤ãƒ«ç­‰ï¼‰ãŒé©åˆ‡ã«è¨­å®šã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹
- é•·æ™‚é–“å®Ÿè¡Œã•ã‚Œã‚‹ã‚»ãƒ«ãŒã‚ã‚‹å ´åˆã¯`--ExecutePreprocessor.timeout`ã®å€¤ã‚’èª¿æ•´ã™ã‚‹
- VS Codeã§é–‹ã„ã¦ã„ã‚‹å ´åˆã¯å®Ÿè¡Œå¾Œã«ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°ã‚’ç¢ºèªã™ã‚‹
```
